{"cells":[{"cell_type":"markdown","metadata":{"id":"zYdaby0Z9r_R"},"source":["referenfce: https://github.com/ukairia777/finance_sentiment_corpus"]},{"cell_type":"markdown","metadata":{"id":"tnD-hOHZhBiW"},"source":["학습 데이터와 검증 데이터를 비교하여 모델의 성능을 평가할 수 있다. \\\n","정확도는 검증 데이터의 정확도를 말한다. \\\n","검증 데이터의 loss와 학습데이터와 검증데이터의 차이를 통해 과적합을 판단할 수 있다. \\\n","검증데이터의 train과 test데이터의 비율, 학습 데이터의 train과 test데이터의 비율을 변경하여 정확도를 높일 수 있다.\\\n","데이터 셋의 감정 비율을 정확하게 나눠서 정확도를 높일 수 있다. "]},{"cell_type":"markdown","metadata":{"id":"HC34SK4xhBiX"},"source":["### Data Load | Train, Valid, Test 데이터 나누기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JA4BmcS79ONE"},"outputs":[],"source":["# !pip install transformers \n","# !pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KnICpQNa-aGn"},"outputs":[],"source":["import numpy as np \n","import pandas as pd "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ZoZhIJA-mbB"},"outputs":[],"source":["# 데이터 로드 >> 데이터 프레임에 저장 \n","# df = pd.read_csv('./model/data1(p1)+data2_re.csv',index_col=0) \n","# print('샘플의 개수:', len(df))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":169,"referenced_widgets":["3a4b368f99d44e5aba79e6b4cec127a2","afdca75b2bca44c9a71e87fdca7d94b0","c119b16a44f64bd18e46a6f7d00b6838","1b5a2bf4fef24d0b9394863b192fae1e","a133c256e0bf4754940cd8e4b04db5c6","f898cfdb728b49dfab5918359e464cde","e95e79d984e74fecbe0d763d344deaeb","d5e682eb0760414ab451eb3ac0cc8d2f","bcb1fa9c08c94d5eaa1a400cf8498b8b","8d2f3dc0623d41e8a7634a2e9012bfd4","31f1fedf21b84d2281fc9410f6ececb4","fd1c54be285e44f3913e4a30d5f5a3bb","cbea0fe6059c4c2db779c283b060a1c3","9f02be3aec3e400cb949d9a173e118c2","9b343480d6904eb6ae1171054403c2a5","163dafdc83b24cd5827ea574c56dbbb3","7a5d43323e9a4a44be56719a03a86069","0d603e81b0b042389a0c889e3366c027","4fe1ccd5fe7a4c4fb028ecb5eb44ece2","0563e671e37e4e23b6e0c1482b01b16e","90628cd248a2457aba40d99d6303809d","073a71b8bc4e4d109a7b57f8d2c64d8b","d0ddb942dcb445e680ffbce9dcefd362","4f8015c553ef44ea993ec86ed6f6b63f","8c6819c9d45c4a05bee553410b1201e2","9f9ad547c84d4eaa8e55045e8642a866","5cceb75034544cd2a6ddd673b5b49726","e59e1f67fc364333a0a16c3c1bd2884f","4d9bb5815ebf474f83cba27ec3dab154","823664c8c0064529bd3491bea8857025","91431b3b50c0452693ab74cd919efb6c","9b33f67153414bcdaa3dffc6bf9a5057","17316f597e7f4fc7894c2e9cbe33d44c","95c314c3afe147ac840222fd365bc3df","f6458090cdce49ac80367659a3214a84","f5cdf4294a2841218d53c1b257e72d40","656b78649915440f8ad5ed95d86b2520","e09c6c39ace34f1f8f836392d7dccbbe","3a139221a02b49fdb51cdcf7c7a9198b","3892edf53c8d40d39b7e098951add2fd","b696a6e758804351aaf544ee2466a2c5","57b060fe165243ddafb76587d21b2f3f","44029067ee9446b59b9a764020f1ab26","4bff243e46cb4a1c85a629a6adfa58db"]},"id":"a-GbxVy7ARfl","outputId":"90e14f24-c532-4080-c211-4b329da5bb50"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset csv/default to C:/Users/SBAuser/.cache/huggingface/datasets/csv/default-8cc7bfd0870e4e86/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"]},{"name":"stderr","output_type":"stream","text":["Downloading data files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n","Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 942.96it/s]\n","                                                        \r"]},{"name":"stdout","output_type":"stream","text":["Dataset csv downloaded and prepared to C:/Users/SBAuser/.cache/huggingface/datasets/csv/default-8cc7bfd0870e4e86/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00, 487.99it/s]\n"]}],"source":["# Pandas를 사용하여 로드하거나 Hugging Face의 datasets 라이브러리를 사용하여 로드할 수 있으며, \n","# 필요에 따라 적절한 방식을 선택하면 됩니다.\n","\n","# load_dataset 모듈 활용 \n","from datasets import load_dataset\n","\n","all_data = load_dataset(\n","    \"csv\",\n","    data_files={\n","        \"train\": \"./data/data1(p1)+data2_re.csv\"\n","    }\n",")\n","\n","# 현재 train에 모든 데이터가 저장됨 \n","# load_dataset() 허깅페이스 사에서 만든 transformer library "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uy_ziwW2AqHA"},"outputs":[],"source":["# dataset 모듈 활용, train/test set 분류 \n","# train_test_split()\n","\n","cs = all_data['train'].train_test_split(0.2)\n","# 8:2 비율로 훈련 데이터, 검증 데이터 구분, 저장 \n","train_cs = cs['train']\n","test_cs = cs['test']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4LHZHYh8Bwa7","outputId":"8c774cbe-7fe8-4d39-b868-bda412a7a46b"},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['labels', 'kor_sentence'],\n","    num_rows: 32983\n","})"]},"execution_count":177,"metadata":{},"output_type":"execute_result"}],"source":["train_cs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p8_SqxK6BxtD","outputId":"de360561-c871-4056-a2de-5aba73ec563c"},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['labels', 'kor_sentence'],\n","    num_rows: 8246\n","})"]},"execution_count":178,"metadata":{},"output_type":"execute_result"}],"source":["test_cs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b_Y6Edn4B0Ji"},"outputs":[],"source":["# 검증 데이터 생성하기 \n","# 훈련용 데이터를 다시 8:2로 구분, 저장 \n","\n","cs = train_cs.train_test_split(0.2)\n","train_cs = cs['train']\n","valid_cs = cs['test']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z_Mf-ZDNCYZ8","outputId":"fd77b695-9c2c-46f9-b1a5-4b7c0074926c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['labels', 'kor_sentence'],\n","    num_rows: 26386\n","})\n","\n","Dataset({\n","    features: ['labels', 'kor_sentence'],\n","    num_rows: 6597\n","})\n","\n","Dataset({\n","    features: ['labels', 'kor_sentence'],\n","    num_rows: 8246\n","})\n"]}],"source":["# 훈련 데이터 \n","print(train_cs)\n","print()\n","print(valid_cs)\n","print()\n","print(test_cs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OszOccYqCfkH","outputId":"eaca1dd5-bf0d-4231-e0f0-5168fd952934"},"outputs":[{"name":"stdout","output_type":"stream","text":["두번째 샘플 출력: 온국민이 억울하다 얘\n","두번째 샘플의 레이블 출력: 2\n"]}],"source":["print('두번째 샘플 출력:', train_cs['kor_sentence'][1])\n","print('두번째 샘플의 레이블 출력:', train_cs['labels'][1])"]},{"cell_type":"markdown","metadata":{"id":"g1KOTYBkDGp3"},"source":["### 데이터 셋 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tW2Ij8V7Ct3G"},"outputs":[],"source":["import numpy as np \n","import pandas as pd\n","import random \n","import time\n","import datetime \n","from tqdm import tqdm \n","\n","import csv \n","import os \n","\n","import tensorflow as tf \n","import torch "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pX92I4QfDV-i"},"outputs":[],"source":["# BERT 사용하기 위한 모듈 불러오기 \n","\n","from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Snmbb1UD_wz"},"outputs":[],"source":["# 패딩(padding) 위한 모듈 불러오기 \n","\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# 전처리 및 평가지표 \n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, hamming_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S4fTQa1yEYCF"},"outputs":[],"source":["# BERT 구조 \n","# [CLS] sequence [SEP] 구조\n","\n","# 훈련 데이터, 검증 데이터, 테스트 데이터를 BERT 구조로 변환 \n","\n","train_sentences = list(map(lambda x: '[CLS]' + str(x) + '[SEP]', train_cs['kor_sentence']))\n","validation_sentences = list(map(lambda x: '[CLS]' + str(x) + '[SEP]', valid_cs['kor_sentence']))\n","test_sentences = list(map(lambda x: '[CLS]' + str(x) + '[SEP]', test_cs['kor_sentence']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bygu-Gg0FNFS"},"outputs":[],"source":["train_labels = train_cs['labels']\n","validation_labels = valid_cs['labels']\n","test_labels = test_cs['labels']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E_5bC2JnFY1H","outputId":"b2ec0da0-d5a2-4ad6-a50e-de94fb49ac91"},"outputs":[{"data":{"text/plain":["['[CLS]아내의 병증이 점점 심해져 누가 항상 옆에서 보살펴 줘야 하는데 방법이 없네.[SEP]',\n"," '[CLS]내가 앓고 있는 공황장애 때문에 부모님이 많이 싸우셔.[SEP]',\n"," '[CLS]건강하게 몸 관리 잘 하시고언제나 그랬듯이 최선을 다하는 모습이 보물 같습니다[SEP]',\n"," '[CLS]언젠가 어디선가 좋은날 계속 사랑하고 있길 바라며안녕 도깨비 [SEP]',\n"," '[CLS]무상기증을 당연시하지마라[SEP]']"]},"execution_count":188,"metadata":{},"output_type":"execute_result"}],"source":["test_sentences[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gDhWfm5zFcc9","outputId":"cfd174da-bdaa-41b4-a660-7e504995c1ff"},"outputs":[{"data":{"text/plain":["[2, 2, 1, 2, 1]"]},"execution_count":143,"metadata":{},"output_type":"execute_result"}],"source":["# 행복 = 0, 슬픔 = 1, 분노 = 2 \n","\n","test_labels[:5]"]},{"cell_type":"markdown","metadata":{"id":"khUyPrdzFqXM"},"source":["BERT 토큰나이저 이용한 전처리 \n","\n","- BERT 사용하기 위해서는 tokenizer와 model이 반드시 mapping 되는 관계여야만 함 >> 아래의 이름에 들어가는 모델이름은 반드시 동일해야 함 \n","- BertTokenizer.from_pretrained('모델이름') \n","- BertForSequenceClassification.from_pretrained('모델이름') \n","\n","- Tokenizer의 역할은 내부적으로 vocabulary를 갖고 있어서 정수 인코딩을 수행해주는 모듈"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w5F2sz2DFmHu"},"outputs":[],"source":["# kor-bert 중 하나니 'klue/bert-base'를 사용 \n","tokenizer = BertTokenizer.from_pretrained('klue/bert-base')\n","\n","# klue/bert-base는 Hugging Face의 Transformers 라이브러리에서 제공하는 한국어 BERT 모델 중 하나입니다. \n","# 해당 모델을 사용하기 위해 BertTokenizer.from_pretrained를 사용하여 토크나이저를 불러올 수 있습니다.\n","\n","# 위 코드를 실행하면 klue/bert-base 모델의 사전을 기반으로 한 토크나이저 객체가 생성됩니다. \n","# 이 토크나이저를 사용하여 텍스트를 토큰화하고, 토큰을 정수로 변환하거나 정수를 토큰으로 변환할 수 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NQqhyp0cGfk9"},"outputs":[],"source":["MAX_LEN = 64\n","# pre-trained model : 최대 길이 512\n","# fine-tuning 할 때 데이터 셋 길이 128\n","\n","def data_to_tensor(sentences, labels): \n","   # 정수 인코딩 과정, 각 텍스트를 토큰화한 후에 vocabulary에 mapping 되는 정수 시퀀스로 변환 \n","   # 예) ['안녕하세요'] >> ['안','녕','하세요'] >> [231,52,45]\n","   tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","   input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","   # pad_sequences: padding 위한 모듈. 주어진 max_len(최대길이)위에 뒤에 0으로 채움 \n","   # 예) [231,52,45] >> [231,52,45,0,0,0]\n","\n","   input_ids = pad_sequences(input_ids, maxlen = MAX_LEN, dtype='long', truncating='post', padding='post')\n","   # truncating은 시퀀스가 최대 길이를 초과할 경우 어느 부분을 삭제할 것인지를 설정합니다. \n","   # 여기서는 'post'로 설정되어 시퀀스의 뒷부분이 잘리게 됩니다. \n","   # padding은 패딩에 사용할 값으로, 'post'로 설정하면 시퀀스의 뒷부분이 패딩 값으로 채워집니다.\n","      \n","   attention_masks =[]\n","\n","   for seq in input_ids:\n","      seq_mask = [float(i>0) for i in seq]\n","      attention_masks.append(seq_mask)\n","      # float(i > 0)를 사용하여 패딩이 아닌 토큰에 대해서는 1, 패딩 토큰에 대해서는 0으로 값을 설정합니다.\n","\n","   # 각각 PyTorch의 torch.tensor로 변환\n","   tensor_inputs = torch.tensor(input_ids)\n","   tensor_labels = torch.tensor(labels)\n","   tensor_masks = torch.tensor(attention_masks)\n","\n","   return tensor_inputs, tensor_labels, tensor_masks\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lkaP9lRFIrLM","outputId":"46df086d-ba83-4b5b-b38c-871ed9e5069a"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\SBAuser\\AppData\\Local\\Temp\\ipykernel_19944\\3465040229.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  tensor_labels = torch.tensor(labels)\n"]}],"source":["# 훈련 데이터, 검증데이터, 테스트 데이터에 대해서 data_to_tensor 함수를 활용\n","# 정수 인코딩된 데이터, 레이블, 어텐션 마스크를 얻음 \n","\n","train_inputs, train_labels, train_masks = data_to_tensor(train_sentences, train_labels)\n","validation_inputs, validation_labels, validation_masks = data_to_tensor(validation_sentences, validation_labels)\n","test_inputs, test_labels, test_masks = data_to_tensor(test_sentences, test_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rNYVRPbIJfcE","outputId":"244cce93-9eef-4ae9-ee13-e0c0053ce0a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([    2,  7740,  1513,  6186, 27321,  2059,  2088,  1041,  2200,  4074,\n","         2085,  1295,  1415,  2069,  3681,  2200,  4093,  2182,     3,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0], dtype=torch.int32)\n","\n","tensor(1)\n","\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"]}],"source":["print(train_inputs[0])\n","print()\n","print(train_labels[0])\n","print()\n","print(train_masks[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9WSrvFhlhBij","outputId":"5c665732-4d0f-4e1c-ac7b-cca6fa1d8383"},"outputs":[{"data":{"text/plain":["<bound method PreTrainedTokenizerBase.decode of BertTokenizer(name_or_path='klue/bert-base', vocab_size=32000, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)>"]},"execution_count":162,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"rSlpuvNtJtm0","outputId":"12172f66-c323-41a1-9001-79ad4ac34a9a"},"outputs":[{"data":{"text/plain":["'[PAD]'"]},"execution_count":165,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode([0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"LyMBSnvOJ5ct","outputId":"0d8c836f-b357-4d57-edf5-6be63e995560"},"outputs":[{"data":{"text/plain":["'[SEP]'"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode([3])"]},{"cell_type":"markdown","metadata":{"id":"2URRp-bmLVOA"},"source":["batch_size = 32 \n","- pytorch의 DataLoader이용, 변환 \n","- DataLoader: batch 단위로 데이터를 꺼내올 수 있도록 하는 모듈 "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JW61GjpYKXBD"},"outputs":[],"source":["batch_size = 32\n","\n","#  PyTorch의 TensorDataset, RandomSampler, 그리고 DataLoader를 사용하여 훈련 데이터를 미니배치로 나눈다.\n","\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","# train_data를 입력으로 받아서 무작위로 샘플링하는 랜덤 샘플러를 생성\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","# DataLoader: train_data, train_sampler, 그리고 batch_size를 입력으로 받아서 데이터 로더를 생성합니다. \n","# 이 데이터 로더는 미니배치로 데이터를 로드하는 역할을 합니다. \n","# train_dataloader를 통해 반복문을 사용하여 미니배치 단위로 데이터를 로드하고 모델을 훈련할 수 있습니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N5y2PEXxQdur"},"outputs":[],"source":["validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = RandomSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JKcrxjv-Qpqh"},"outputs":[],"source":["test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","test_sampler = RandomSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9NXfBaZxQtCa","outputId":"e7637efc-6f1e-4e17-a5c4-acf606984612"},"outputs":[{"name":"stdout","output_type":"stream","text":["훈련 데이터 크기: 26386\n","검증 데이터 크기: 6597\n","테스트 데이터 크기: 8246\n"]}],"source":["print('훈련 데이터 크기:', len(train_labels))\n","print('검증 데이터 크기:', len(validation_labels))\n","print('테스트 데이터 크기:', len(test_labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KHE-IT6hQ5jX","outputId":"700098fd-deb1-455a-d4b7-dc2d0a5530d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: NVIDIA GeForce RTX 3070\n"]}],"source":["# GPU 정상 세팅이 되었는지 확인 \n","\n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No GPU available, using the CPU instead.')"]},{"cell_type":"markdown","metadata":{"id":"dKCOLpeDRKpa"},"source":["### 모델 로드하기 \n","\n","- BERT 사용, 텍스트 분류 아키텍처\n","  - BertForSequenceClassification.from_pretrained('모델명') \n","  - label 개수: num_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":962,"referenced_widgets":["3ff6ae34901b42a0b14b55e0e2fdd698","3d437519e34d4205806651e0f7566380","21ea6c1fe24c48b9a557e78fddef1898","36ae18d026164f379d4ffda2779a941d","6125e69b9b404208b3a88f98a038cfd6","7769b5b29b494710a04b89948fe96c53","25fc09a6ed1446e9a50f7034abf7972a","cef45b7fb6874cfba1a34f8de24a496e","81af89da2512434ba95893566b29fe01","4d04ecefd3d847388170c6113897e6a3","86e08b53aa23444ea9f2faf2f535797d"]},"id":"J9XzuoWlRJgA","outputId":"27f004df-5d23-4646-8949-a4436029b138"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",")"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["num_labels = 3\n","\n","model = BertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=num_labels)\n","model.cuda()\n","\n","#  \"klue/bert-base\" 사전 훈련된 BERT 모델을 불러와 시퀀스 분류 작업을 위한 모델 객체를 생성하고, \n","# 해당 모델을 GPU로 이동\n","#  \"klue/bert-base\"는 사전 훈련된 BERT 모델의 이름\n","# 모델을 GPU로 이동시키기 위해 model.cuda()를 호출\n","# 이를 통해 모델의 모든 매개변수와 버퍼가 GPU 메모리에 할당"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yUyk0HV6RJip","outputId":"48ef74f5-5f44-41bc-cff3-982c1fd1f2f7"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\SBAuser\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["# 최적화(optimizer) 선택 \n","\n","optimizer = AdamW(model.parameters(),\n","            lr= 2e-5,\n","            eps=1e-8)\n","\n","# 학습 반복 횟수 설정 (epochs)\n","\n","epochs = 2\n","total_steps = len(train_dataloader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                num_warmup_steps = 0, \n","                                num_training_steps = total_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FzRNhABkUL53"},"outputs":[],"source":["# 훈련 과정에서 경과 시간을 형식화\n","def format_time(elapsed): \n","   elapsed_rounded = int(round(elapsed))\n","   return str(datetime.timedelta(seconds=elapsed_rounded)) \n","   # hh:mm:ss "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DQdLDJxyUmdj"},"outputs":[],"source":["def metrics(predictions, labels): \n","   y_pred = predictions\n","   y_true = labels\n","\n","   # 사용 가능한 메트릭들을 사용함 \n","   accuracy = accuracy_score(y_true, y_pred)\n","   # accuracy_score 함수\n","   # y_true와 y_pred 두 개의 배열을 입력으로 받아 정확도를 계산\n","   f1_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0) \n","   # 'macro'는 클래스별 F1 스코어의 평균을 계산\n","   f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0) \n","   # 'micro'는 전체 예측 결과에 대한 F1 스코어를 계산\n","   f1_weighted_average = f1_score(y_true=y_true, y_pred=y_pred, average='weighted', zero_division=0)\n","   # 'weighted'는 클래스별 샘플 수로 가중 평균된 F1 스코어를 계산\n","   # zero_division 매개변수는 분모가 0일 경우 처리 방법을 지정\n","   \n","   # metric 결과에 대해서도 return \n","\n","   metrics = {'accuracy': accuracy, \n","              'f1_macro': f1_macro_average,\n","              'f1_micro': f1_micro_average,\n","              'f1_weighted': f1_weighted_average\n","              }\n","\n","   return metrics"]},{"cell_type":"markdown","metadata":{"id":"mlLKd6KKW1rl"},"source":["### 모델 학습 "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zV34moAfWrkK","outputId":"92d60170-e153-46c8-cda7-907566982388"},"outputs":[{"name":"stdout","output_type":"stream","text":["---------- Epoch 1 / 5 ----------\n"]},{"name":"stderr","output_type":"stream","text":["501it [01:22,  6.08it/s]"]},{"name":"stdout","output_type":"stream","text":[" Batch   500 of   825, Elapsed: 0:01:22.\n"]},{"name":"stderr","output_type":"stream","text":["825it [02:15,  6.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Average training loss: 0.5078\n","Training epoch taken: 0:02:15\n","Accuracy: 78.41%\n","---------- Epoch 2 / 5 ----------\n"]},{"name":"stderr","output_type":"stream","text":["501it [01:21,  6.05it/s]"]},{"name":"stdout","output_type":"stream","text":[" Batch   500 of   825, Elapsed: 0:01:22.\n"]},{"name":"stderr","output_type":"stream","text":["825it [02:14,  6.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Average training loss: 0.3565\n","Training epoch taken: 0:02:14\n","Accuracy: 85.66%\n","---------- Epoch 3 / 5 ----------\n"]},{"name":"stderr","output_type":"stream","text":["501it [01:21,  6.07it/s]"]},{"name":"stdout","output_type":"stream","text":[" Batch   500 of   825, Elapsed: 0:01:21.\n"]},{"name":"stderr","output_type":"stream","text":["825it [02:13,  6.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Average training loss: 0.2579\n","Training epoch taken: 0:02:13\n","Accuracy: 90.03%\n","---------- Epoch 4 / 5 ----------\n"]},{"name":"stderr","output_type":"stream","text":["501it [01:20,  6.24it/s]"]},{"name":"stdout","output_type":"stream","text":[" Batch   500 of   825, Elapsed: 0:01:21.\n"]},{"name":"stderr","output_type":"stream","text":["825it [02:12,  6.21it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Average training loss: 0.1865\n","Training epoch taken: 0:02:13\n","Accuracy: 93.14%\n","---------- Epoch 5 / 5 ----------\n"]},{"name":"stderr","output_type":"stream","text":["501it [01:21,  6.27it/s]"]},{"name":"stdout","output_type":"stream","text":[" Batch   500 of   825, Elapsed: 0:01:21.\n"]},{"name":"stderr","output_type":"stream","text":["825it [02:13,  6.20it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Average training loss: 0.1373\n","Training epoch taken: 0:02:13\n","Accuracy: 95.21%\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# random seed 값 설정 \n","\n","seed_val = 777\n","# seed_val에 원하는 시드 값을 지정합니다.\n","random.seed(seed_val)\n","# 파이썬의 random 모듈에 시드 값을 설정\n","np.random.seed(seed_val)\n","# NumPy의 랜덤 모듈에 시드 값을 설정\n","torch.manual_seed(seed_val)\n","# PyTorch의 랜덤 모듈에 시드 값을 설정\n","torch.cuda.manual_seed_all(seed_val)\n","# CUDA 디바이스를 사용하는 경우, 모든 CUDA 디바이스에 시드 값을 설정\n","\n","\n","model.zero_grad()\n","# model.zero_grad()는 모델의 모든 가중치에 대한 그래디언트를 0으로 초기화하는 작업을 수행\n","# 딥러닝 모델을 학습할 때, 역전파(backpropagation) 과정에서 이전에 계산된 그래디언트 값들이 누적되는 경우가 있을 수 있습니다. 이를 방지하기 위해 매 학습 반복(iteration)마다 그래디언트를 초기화해주어야 합니다.\n","# model.zero_grad()는 모델의 parameters() 메서드를 통해 반환된 모든 매개변수들의 그래디언트를 0으로 설정합니다. \n","# 이를 통해 다음 학습 반복에서 새로운 그래디언트 값을 계산할 수 있습니다.\n","\n","for epoch_i in range(0, epochs): \n","  print('---------- Epoch {:} / {:} ----------'.format(epoch_i + 1, epochs))\n","  t0 = time.time() # 현재 epoch의 시작 시간을 기록\n","  total_loss = 0 #  현재 epoch의 총 손실(loss) 값을 초기화\n","  total_correct = 0  # 정확한 예측의 개수를 추적하기 위한 변수\n","  \n","  model.train() #  모델을 학습 모드로 설정\n","\n","  for step, batch in tqdm(enumerate(train_dataloader)): \n","    if step % 500 == 0 and not step == 0: \n","      elapsed = format_time(time.time() - t0)\n","      print(' Batch {:>5,} of {:>5,}, Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","    batch = tuple(t.to(device) for t in batch)     \n","    b_input_ids, b_input_mask, b_labels = batch\n","\n","    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","    # model은 BERT 모델 객체\n","    #  attention_mask는 패딩을 위한 마스크를 나타내는 인자\n","    #  labels는 모델의 출력과 비교하여 손실을 계산하기 위한 정답 레이블\n","    loss = outputs[0]\n","    total_loss += loss.item()\n","    loss.backward()\n","    \n","    # 정확도 계산\n","    logits = outputs[1]  # 모델의 예측값 (로짓)\n","    preds = torch.argmax(logits, dim=1)  # 가장 높은 로짓 값을 갖는 클래스로 예측\n","    total_correct += torch.sum(preds == b_labels).item()  # 정확한 예측의 개수 계산\n","\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n","    # threshold 초과시, gradient clipping 해줌 \n","    optimizer.step()\n","    scheduler.step()\n","\n","    model.zero_grad()\n","\n","  avg_train_loss = total_loss / len(train_dataloader)\n","  accuracy = total_correct / len(train_dataloader.dataset)  # 전체 데이터셋에 대한 정확도 계산\n","\n","\n","  print(\"\")\n","  print(\"Average training loss: {0:.4f}\".format(avg_train_loss))\n","  print(\"Training epoch taken: {:}\".format(format_time(time.time() - t0)))\n","  print(\"Accuracy: {:.2%}\".format(accuracy))\n"]},{"cell_type":"markdown","metadata":{"id":"RP5RSULjaBWf"},"source":["### 검증 데이터에 대한 평가 "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OKMA6xYAZzh7","outputId":"0dfd8ea5-cd18-40b4-bd57-1c0ccad69a4f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1 1 0 ... 1 1 1]\n","[1 1 0 ... 1 1 1]\n","{'accuracy': 0.8129452781567379, 'f1_macro': 0.8190171966085233, 'f1_micro': 0.8129452781567379, 'f1_weighted': 0.8128970229544441}\n","Accuracy: 0.8129\n","f1_macro score: 0.8190\n","f1_micro score: 0.8129\n","f1_weighted score: 0.8129\n"]}],"source":["t0 = time.time()\n","# eval() : 파이썬에서 문자열로 된 코드 실행 함수 \n","# eval(\"1+2\")\n","model.eval() \n","accum_logits, accum_label_ids = [], []\n","\n","for batch in validation_dataloader: \n","  batch = tuple(t.to(device) for t in batch) \n","  b_input_ids, b_input_mask, b_labels = batch\n","  #  validation_dataloader에서 반복하면서 검증 데이터 배치를 가져오고, 각 텐서를 GPU로 이동\n","  # (to(device)를 사용하여 디바이스를 지정\n","\n","  with torch.no_grad(): \n","    outputs = model(b_input_ids, \n","                    token_type_ids=None,\n","                    attention_mask = b_input_mask)\n","    \n","    # print(outputs[0])\n","\n","    logits = outputs[0]\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    for b in logits:\n","      # 3개의 값 중 가장 큰 값을 예측한 인덱스로 결정 \n","      # 예를 들어 [3.5134, - 0.308765, -2.11316] >> 0\n","      accum_logits.append(np.argmax(b))\n","\n","    for b in label_ids: \n","      accum_label_ids.append(b)\n","  \n","accum_logits = np.array(accum_logits)\n","accum_label_ids = np.array(accum_label_ids)\n","results = metrics(accum_logits, accum_label_ids)\n","\n","print(accum_logits)\n","print(accum_label_ids)\n","print(results)\n","\n","print(\"Accuracy: {0:.4f}\".format(results['accuracy']))\n","print(\"f1_macro score: {0:.4f}\".format(results['f1_macro']))\n","print(\"f1_micro score: {0:.4f}\".format(results['f1_micro']))\n","print(\"f1_weighted score: {0:.4f}\".format(results['f1_weighted']))"]},{"cell_type":"markdown","metadata":{"id":"0KvrLMltfbIq"},"source":["### 모델 저장과 모델 로드 "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ky7PIEzZcnYb","outputId":"eac9fa59-8f63-4bc1-ce5b-083187740c30"},"outputs":[{"name":"stderr","output_type":"stream","text":["���� ���͸� �Ǵ� ���� model��(��) �̹� �ֽ��ϴ�.\n"]}],"source":["# 폴더 생성 \n","%mkdir model "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"FbcdF3gnfonh","outputId":"6eaaa4f7-88ad-45a3-919a-6b6916af7a7b"},"outputs":[{"data":{"text/plain":["'c:\\\\Users\\\\SBAuser\\\\Desktop\\\\Team_project_Toon_NLP\\\\Team_project_Toon_NLP'"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WRft1Pk7dLuk"},"outputs":[],"source":["path = './model/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Rz6xk4tfrFV"},"outputs":[],"source":["# 모델 저장 \n","torch.save(model.state_dict(), \"./model/bert_multi_sentiment.pt\")\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# 모델 로드 \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39m./bert_multi_sentiment.pt\u001b[39m\u001b[39m\"\u001b[39m))\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["# 모델 로드 \n","model.load_state_dict(torch.load(\"./bert_multi_sentiment.pt\"))"]},{"cell_type":"markdown","metadata":{"id":"tgMkQzVDggUv"},"source":["### 테스트 데이터에 대한 평가 "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7vALohaWgddu","outputId":"72c6072c-243b-4625-d0cd-be28c9404cbf"},"outputs":[{"name":"stderr","output_type":"stream","text":["102it [00:04, 22.97it/s]"]},{"name":"stdout","output_type":"stream","text":["Batch   100 of   258, Elapsed: 0:00:04.\n"]},{"name":"stderr","output_type":"stream","text":["204it [00:08, 22.98it/s]"]},{"name":"stdout","output_type":"stream","text":["Batch   200 of   258, Elapsed: 0:00:09.\n"]},{"name":"stderr","output_type":"stream","text":["258it [00:11, 22.95it/s]"]},{"name":"stdout","output_type":"stream","text":["[2 0 2 ... 2 0 0]\n","[1 0 1 ... 2 0 0]\n","{'accuracy': 0.8114237205918021, 'f1_macro': 0.8160182756587008, 'f1_micro': 0.8114237205918021, 'f1_weighted': 0.8117379977061263}\n","Accuracy: 0.8114\n","f1_macro score: 0.8160\n","f1_micro score: 0.8114\n","f1_weighted score: 0.8117\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["t0 = time.time()\n","model.eval()\n","accum_logits, accum_label_ids = [],[]\n","\n","for step, batch in tqdm(enumerate(test_dataloader)):\n","  if step % 100 == 0 and not step == 0: \n","    elapsed = format_time(time.time() - t0)\n","    print('Batch {:>5,} of {:>5,}, Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n","\n","  batch = tuple(t.to(device) for t in batch)\n","  b_input_ids, b_input_mask, b_labels = batch\n","\n","  with torch.no_grad():\n","    outputs = model(b_input_ids, \n","                    token_type_ids=None, \n","                    attention_mask = b_input_mask)\n","  logits = outputs[0]\n","  logits = logits.detach().cpu().numpy()  \n","  label_ids = b_labels.to('cpu').numpy()\n","\n","  for b in logits: \n","    accum_logits.append(np.argmax(b))\n","\n","  for b in label_ids: \n","    accum_label_ids.append(b)\n","\n","accum_logits = np.array(accum_logits)\n","accum_label_ids = np.array(accum_label_ids)\n","results = metrics(accum_logits, accum_label_ids)\n","\n","print(accum_logits)\n","print(accum_label_ids)\n","print(results)\n","\n","print(\"Accuracy: {0:.4f}\".format(results['accuracy']))\n","print(\"f1_macro score: {0:.4f}\".format(results['f1_macro']))\n","print(\"f1_micro score: {0:.4f}\".format(results['f1_micro']))\n","print(\"f1_weighted score: {0:.4f}\".format(results['f1_weighted']))\n"]},{"cell_type":"markdown","metadata":{"id":"VhnZQywRiPEn"},"source":["### 예측 "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8WASg8cQiHen","outputId":"baa8a1b6-02c6-44a4-ba75-7870685b79be"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\SBAuser\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n","  warnings.warn(\n"]},{"data":{"text/plain":["<transformers.pipelines.text_classification.TextClassificationPipeline at 0x1e2d6068250>"]},"execution_count":170,"metadata":{},"output_type":"execute_result"}],"source":["# from transformers import pipeline\n","# pipeline : 정형화된 데이터 일 경우 사용 \n","\n","# return_all_scores=True : 모든 라벨에 대한 확률값 반환 \n","# pipe = pipeline('text-classification', model = model.cuda(), tokenizer = tokenizer, device=0, max_length=512, \n","#                 return_all_scores=True, function_to_apply='softmax' )\n","# pipe\n","# result = pipe('화난다')\n","# print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Mn58FRi4VzP"},"outputs":[],"source":["from transformers import pipeline\n","pipe = pipeline('text-classification', model = model.cuda(), tokenizer = tokenizer, device=0, max_length=512, \n","                 function_to_apply='softmax')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"htrO3kjm4cdp","outputId":"79713417-261d-488d-ac14-18d58f6b6a7e"},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'label': 'LABEL_0', 'score': 0.9661669135093689}]\n"]}],"source":["result = pipe('SK하이닉스가 매출이 급성장하였다')\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zreEe6RPiyDx"},"outputs":[],"source":["label_dict = {'LABEL_0': '기쁨', 'LABEL_1':'슬픔', 'LABEL_2':'분노'}\n","label_dict['LABEL_0']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NqRGGsPCiyFm"},"outputs":[],"source":["def prediction(text): \n","   result = pipe(text)\n","\n","   return label_dict[result[0]['label']]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3qzQJppKjxA4","outputId":"9bd56a97-7c3e-42f9-b026-4f545ecd1b42"},"outputs":[{"ename":"TypeError","evalue":"list indices must be integers or slices, not str","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[193], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m prediction(\u001b[39m\"\u001b[39;49m\u001b[39m화난다\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n","Cell \u001b[1;32mIn[65], line 4\u001b[0m, in \u001b[0;36mprediction\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprediction\u001b[39m(text): \n\u001b[0;32m      2\u001b[0m    result \u001b[39m=\u001b[39m pipe(text)\n\u001b[1;32m----> 4\u001b[0m    \u001b[39mreturn\u001b[39;00m [label_dict[result[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m]]]\n","\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"]}],"source":["prediction(\"화난다\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8kB3xHoXj6Rf","outputId":"dd482728-8abe-461e-9419-bf5806eac1f4"},"outputs":[{"data":{"text/plain":["['슬픔']"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["prediction(\"나 너무 슬퍼 ㅠㅠ\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EOWP_rsjhBi8","outputId":"30ec6720-6e67-4202-95b4-65ed58468793"},"outputs":[{"data":{"text/plain":["['기쁨']"]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["prediction(\"와 기분이 너무 좋아!!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ynsGT_oMhBi8","outputId":"16707e0b-e59e-43f2-8a79-070a4cce72d9"},"outputs":[{"data":{"text/plain":["['기쁨']"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["prediction(\"오늘 행복해 \")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4SVM-26ghBi9","outputId":"1ef06cc9-5bb0-4a4c-da6b-5aa98f14315c"},"outputs":[{"data":{"text/plain":["['슬픔']"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["prediction(\"시험을 망쳤어 우울해\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6aylmd0EhBi9","outputId":"086145c8-cd1b-4c3d-a095-cc09d2f35541"},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'label': 'LABEL_2', 'score': 0.9934315085411072}]\n"]}],"source":["result = pipe('호미리 깨물어 주고 싶어')\n","print(result)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0563e671e37e4e23b6e0c1482b01b16e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"073a71b8bc4e4d109a7b57f8d2c64d8b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d603e81b0b042389a0c889e3366c027":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"163dafdc83b24cd5827ea574c56dbbb3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17316f597e7f4fc7894c2e9cbe33d44c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b5a2bf4fef24d0b9394863b192fae1e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d2f3dc0623d41e8a7634a2e9012bfd4","placeholder":"​","style":"IPY_MODEL_31f1fedf21b84d2281fc9410f6ececb4","value":" 1/1 [00:00&lt;00:00, 48.68it/s]"}},"21ea6c1fe24c48b9a557e78fddef1898":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cef45b7fb6874cfba1a34f8de24a496e","max":445025130,"min":0,"orientation":"horizontal","style":"IPY_MODEL_81af89da2512434ba95893566b29fe01","value":445025130}},"25fc09a6ed1446e9a50f7034abf7972a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31f1fedf21b84d2281fc9410f6ececb4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36ae18d026164f379d4ffda2779a941d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d04ecefd3d847388170c6113897e6a3","placeholder":"​","style":"IPY_MODEL_86e08b53aa23444ea9f2faf2f535797d","value":" 445M/445M [00:02&lt;00:00, 223MB/s]"}},"3892edf53c8d40d39b7e098951add2fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a139221a02b49fdb51cdcf7c7a9198b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a4b368f99d44e5aba79e6b4cec127a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_afdca75b2bca44c9a71e87fdca7d94b0","IPY_MODEL_c119b16a44f64bd18e46a6f7d00b6838","IPY_MODEL_1b5a2bf4fef24d0b9394863b192fae1e"],"layout":"IPY_MODEL_a133c256e0bf4754940cd8e4b04db5c6"}},"3d437519e34d4205806651e0f7566380":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7769b5b29b494710a04b89948fe96c53","placeholder":"​","style":"IPY_MODEL_25fc09a6ed1446e9a50f7034abf7972a","value":"Downloading pytorch_model.bin: 100%"}},"3ff6ae34901b42a0b14b55e0e2fdd698":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3d437519e34d4205806651e0f7566380","IPY_MODEL_21ea6c1fe24c48b9a557e78fddef1898","IPY_MODEL_36ae18d026164f379d4ffda2779a941d"],"layout":"IPY_MODEL_6125e69b9b404208b3a88f98a038cfd6"}},"44029067ee9446b59b9a764020f1ab26":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bff243e46cb4a1c85a629a6adfa58db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d04ecefd3d847388170c6113897e6a3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d9bb5815ebf474f83cba27ec3dab154":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f8015c553ef44ea993ec86ed6f6b63f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e59e1f67fc364333a0a16c3c1bd2884f","placeholder":"​","style":"IPY_MODEL_4d9bb5815ebf474f83cba27ec3dab154","value":"Generating train split: "}},"4fe1ccd5fe7a4c4fb028ecb5eb44ece2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57b060fe165243ddafb76587d21b2f3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5cceb75034544cd2a6ddd673b5b49726":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"6125e69b9b404208b3a88f98a038cfd6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"656b78649915440f8ad5ed95d86b2520":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44029067ee9446b59b9a764020f1ab26","placeholder":"​","style":"IPY_MODEL_4bff243e46cb4a1c85a629a6adfa58db","value":" 1/1 [00:00&lt;00:00, 43.42it/s]"}},"7769b5b29b494710a04b89948fe96c53":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a5d43323e9a4a44be56719a03a86069":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81af89da2512434ba95893566b29fe01":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"823664c8c0064529bd3491bea8857025":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"86e08b53aa23444ea9f2faf2f535797d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c6819c9d45c4a05bee553410b1201e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_823664c8c0064529bd3491bea8857025","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_91431b3b50c0452693ab74cd919efb6c","value":1}},"8d2f3dc0623d41e8a7634a2e9012bfd4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90628cd248a2457aba40d99d6303809d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91431b3b50c0452693ab74cd919efb6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"95c314c3afe147ac840222fd365bc3df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f6458090cdce49ac80367659a3214a84","IPY_MODEL_f5cdf4294a2841218d53c1b257e72d40","IPY_MODEL_656b78649915440f8ad5ed95d86b2520"],"layout":"IPY_MODEL_e09c6c39ace34f1f8f836392d7dccbbe"}},"9b33f67153414bcdaa3dffc6bf9a5057":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b343480d6904eb6ae1171054403c2a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_90628cd248a2457aba40d99d6303809d","placeholder":"​","style":"IPY_MODEL_073a71b8bc4e4d109a7b57f8d2c64d8b","value":" 1/1 [00:00&lt;00:00, 54.47it/s]"}},"9f02be3aec3e400cb949d9a173e118c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fe1ccd5fe7a4c4fb028ecb5eb44ece2","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0563e671e37e4e23b6e0c1482b01b16e","value":1}},"9f9ad547c84d4eaa8e55045e8642a866":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b33f67153414bcdaa3dffc6bf9a5057","placeholder":"​","style":"IPY_MODEL_17316f597e7f4fc7894c2e9cbe33d44c","value":" 0/0 [00:00&lt;?, ? examples/s]"}},"a133c256e0bf4754940cd8e4b04db5c6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afdca75b2bca44c9a71e87fdca7d94b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f898cfdb728b49dfab5918359e464cde","placeholder":"​","style":"IPY_MODEL_e95e79d984e74fecbe0d763d344deaeb","value":"Downloading data files: 100%"}},"b696a6e758804351aaf544ee2466a2c5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcb1fa9c08c94d5eaa1a400cf8498b8b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c119b16a44f64bd18e46a6f7d00b6838":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5e682eb0760414ab451eb3ac0cc8d2f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bcb1fa9c08c94d5eaa1a400cf8498b8b","value":1}},"cbea0fe6059c4c2db779c283b060a1c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a5d43323e9a4a44be56719a03a86069","placeholder":"​","style":"IPY_MODEL_0d603e81b0b042389a0c889e3366c027","value":"Extracting data files: 100%"}},"cef45b7fb6874cfba1a34f8de24a496e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0ddb942dcb445e680ffbce9dcefd362":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4f8015c553ef44ea993ec86ed6f6b63f","IPY_MODEL_8c6819c9d45c4a05bee553410b1201e2","IPY_MODEL_9f9ad547c84d4eaa8e55045e8642a866"],"layout":"IPY_MODEL_5cceb75034544cd2a6ddd673b5b49726"}},"d5e682eb0760414ab451eb3ac0cc8d2f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e09c6c39ace34f1f8f836392d7dccbbe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e59e1f67fc364333a0a16c3c1bd2884f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e95e79d984e74fecbe0d763d344deaeb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f5cdf4294a2841218d53c1b257e72d40":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b696a6e758804351aaf544ee2466a2c5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_57b060fe165243ddafb76587d21b2f3f","value":1}},"f6458090cdce49ac80367659a3214a84":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a139221a02b49fdb51cdcf7c7a9198b","placeholder":"​","style":"IPY_MODEL_3892edf53c8d40d39b7e098951add2fd","value":"100%"}},"f898cfdb728b49dfab5918359e464cde":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd1c54be285e44f3913e4a30d5f5a3bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cbea0fe6059c4c2db779c283b060a1c3","IPY_MODEL_9f02be3aec3e400cb949d9a173e118c2","IPY_MODEL_9b343480d6904eb6ae1171054403c2a5"],"layout":"IPY_MODEL_163dafdc83b24cd5827ea574c56dbbb3"}}}}},"nbformat":4,"nbformat_minor":0}
